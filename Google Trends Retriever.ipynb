{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EDIT: As of now, you can use proxies to bypass the request limit like <br></br>\n",
    "    pytrends = TrendReq(hl='en-US', tz=360, proxies = {'https': 'https://34.203.233.13:80'})\n",
    "\n",
    "You can only retrieve 1600 requests per day right now, and you need to figure out how much data you will need to collect and how many computers you need to use. I found that I needed one computer:one ip address for about 3200 requests I needed (2992 Dailies and 176 Weekly = 3168) AND you had to download it on the same day else data would not be accurate. Since Google Trends is based on UTC time, you can use this website (https://time.is/UTC) to check that you know your data will be accurate because it will be downloaded in the same timeframe. When you download the data, you will need to reference to this time and not any other time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Daily RETRIEVER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "from datetime import datetime, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Google Authentication \n",
    "pytrend = TrendReq()\n",
    "\n",
    "#Terms you are going to search from - ideally you can read this from a file and put it into a list instead of manually\n",
    "search_terms = ['Red Dead Redemption', 'fortnite', 'Spiderman']\n",
    "\n",
    "#What sub-regions you are looking for - if any\n",
    "sub_regions = ['NZ-AUK', 'NZ-CAN', 'NZ-WGN']\n",
    "\n",
    "#lists\n",
    "time_from = [] #time from this date\n",
    "time_to = [] #time to this date\n",
    "\n",
    "timeframe = []\n",
    "\n",
    "#I generated daily dates to a txt so i could read them here and get data between 90 days\n",
    "#most likely a way easier way to do it like pandas.date_range\n",
    "with open('fromDate.txt', 'r') as fromFile, open ('toDate.txt', 'r') as toFile:\n",
    "    next(fromFile)\n",
    "    next(toFile)\n",
    "    for i in fromFile:\n",
    "        time_from.append(i.strip())\n",
    "        \n",
    "    for j in toFile:\n",
    "        time_to.append(j.strip())\n",
    "    \n",
    "\n",
    "#NO OVERLAP - output is eg. '2013-03-25 2013-06-30'\n",
    "#Have to do it in 90 day splits to get Daily\n",
    "for i in range(len(time_from)):\n",
    "    timeframe.append(str(time_from[i]) + ' ' + str(time_to[i]))\n",
    "    \n",
    "#Pytrends stuff\n",
    "geoCount = 0\n",
    "cat = 41 #Category 41 is Gaming https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories\n",
    "\n",
    "geo = sub_regions[geoCount] #Cities in NZ\n",
    "gprop = ''\n",
    "\n",
    "#Can only have a maximum of 5 terms each request so I take 5 terms each time\n",
    "chunks = [search_terms[x:x+5] for x in xrange(0, len(search_terms), 5)] \n",
    "\n",
    "for j in range(len(timeframe)/2): #This was to get 2013-2014\n",
    "    # for j in range((len(timeframe)/2), (len(timeframe))): #This was to get second half of time period 2015-2016 - I uncommented this after 2013-2014 was finished\n",
    "    #As of now, you can use proxies to bypass the request limit\n",
    "    print 'Timeframe: ' + str(timeframe[j])\n",
    " \n",
    "    for x in range(len(sub_regions)):\n",
    "        print('Sub-Region: ' + str(sub_regions[x]))\n",
    "        count = 0\n",
    "        for i in range(len(chunks)):\n",
    "            time.sleep(random.randint(1,3))#letting it sleep a bit so Google Servers dont get mad at requests I make\n",
    "            pytrend.build_payload(chunks[i], cat, timeframe[j], sub_regions[x], gprop)\n",
    "            interest_over_time_df = pytrend.interest_over_time()\n",
    "            filename = sub_regions[x] + ' ' + timeframe[j] + '-' + str(count) +'.csv'\n",
    "            \n",
    "            #Write to files - Need to manually combine later on\n",
    "            interest_over_time_df.to_csv(filename)\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "print '*****Complete*****'  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h2>Weekly RETRIEVER</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytrends.request import TrendReq\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "# Google Authentication \n",
    "pytrend = TrendReq()\n",
    "\n",
    "#Terms you are going to search from - ideally you can read this from a file and put it into a list instead of manually\n",
    "search_terms = ['Red Dead Redemption', 'fortnite', 'Spiderman']\n",
    "\n",
    "#What sub-regions you are looking for - if any\n",
    "sub_regions = ['NZ-AUK', 'NZ-CAN', 'NZ-WGN']\n",
    "\n",
    "timeframe = ['2012-12-30 2016-12-31'] #indicate your weekly timeframe #Have to do it in 90 day splits to get Daily\n",
    "\n",
    "#Pytrends stuff\n",
    "geoCount = 0\n",
    "cat = 41 #Category 41 is Gaming https://github.com/pat310/google-trends-api/wiki/Google-Trends-Categories\n",
    "\n",
    "geo = sub_regions[geoCount] #Cities in NZ\n",
    "gprop = ''\n",
    "\n",
    "#Can only have a maximum of 5 terms each request so I take 5 terms each time\n",
    "chunks = [search_terms[x:x+5] for x in xrange(0, len(search_terms), 5)] \n",
    "\n",
    "for j in range(len(timeframe)): #This was to get 2013-2014\n",
    "    print 'Timeframe: ' + str(timeframe[j])\n",
    " \n",
    "    for x in range(len(sub_regions)):\n",
    "        print('Sub-Region: ' + str(sub_regions[x]))\n",
    "        count = 0\n",
    "        for i in range(len(chunks)):\n",
    "            time.sleep(random.randint(1,3))#letting it sleep a bit so Google Servers dont get mad at requests I make\n",
    "            pytrend.build_payload(chunks[i], cat, timeframe[j], sub_regions[x], gprop)\n",
    "            interest_over_time_df = pytrend.interest_over_time()\n",
    "            filename = sub_regions[x] + ' ' + timeframe[j] + '-' + str(count) +'.csv'\n",
    "            \n",
    "            #Write to files - Need to manually combine....\n",
    "            interest_over_time_df.to_csv(filename)\n",
    "            \n",
    "            count += 1\n",
    "\n",
    "print '*****Complete*****'  \n",
    "      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
